{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "index_maker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1cWKqfUvhWwpXV-Yn6njrBB10r47HA9Zx",
      "authorship_tag": "ABX9TyMDXxlEqrxEAA1EsYxk6Z1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/wikisearch/blob/master/index_maker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrF0dSI-HlbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "050db4a5-7fbb-48f4-ca36-d118c0032af2"
      },
      "source": [
        "import time\n",
        "start = time.time();\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import xml.sax\n",
        "import subprocess\n",
        "from copy import copy\n",
        "\n",
        "import tqdm\n",
        "import pickle\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from nltk.tokenize.regexp import regexp_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from operator import itemgetter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LozY67HOImur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(txt):\n",
        "  txt = txt.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "  punc_list = '!\"#$&*+,-./;?@\\^_~0123456789'\n",
        "  t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
        "  txt = txt.translate(t)\n",
        "  t = str.maketrans(dict.fromkeys(\"'`\", \"\"))\n",
        "  txt = txt.translate(t)\n",
        "\n",
        "  return txt\n",
        "\n",
        "def regtok(txt):\n",
        "  txt = clean(txt)\n",
        "  tokens = regexp_tokenize(txt, pattern = '\\s+', gaps = True)\n",
        "  return tokens"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eok0fSowW2pC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9592b39d-ee63-4dca-c4c9-5ea7bb3faf80"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecRA94P1bmkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Template: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c\n",
        "\n",
        "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
        "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
        "    def __init__(self):\n",
        "        xml.sax.handler.ContentHandler.__init__(self)\n",
        "        self._buffer = None\n",
        "        self._values = {}\n",
        "        self._current_tag = None\n",
        "        self._pages = []\n",
        "\n",
        "    def characters(self, content):\n",
        "        \"\"\"Characters between opening and closing tags\"\"\"\n",
        "        if self._current_tag:\n",
        "            self._buffer.append(content)\n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        \"\"\"Opening tag of element\"\"\"\n",
        "        if name in ('title', 'text'):\n",
        "            self._current_tag = name\n",
        "            self._buffer = []\n",
        "\n",
        "    def endElement(self, name):\n",
        "        \"\"\"Closing tag of element\"\"\"\n",
        "        if name == self._current_tag:\n",
        "            self._values[name] = ' '.join(self._buffer)\n",
        "\n",
        "        if name == 'page':\n",
        "            self._pages.append((self._values['title'], self._values['text']))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcg3T7IfXu1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c502cda8-26a5-4455-b314-0f7be77c4c43"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stopword = stopwords.words('english')\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "indexed_dict = {}\n",
        "doc_id = 0\n",
        "output_path = sys.argv[2]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osCO-7VxohJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_dict = {}\n",
        "\n",
        "def stem(token):\n",
        "  if token in stemmed_dict:\n",
        "    return stemmed_dict[token]\n",
        "  temp = snowball_stemmer.stem(token)\n",
        "  stemmed_dict[token] = temp\n",
        "  return temp"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Oy8I7oVI8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# title: 0, infobox: 1, body: 2, categories: 3, references: 4, external_links: 5\n",
        "\n",
        "class NewHandler(xml.sax.ContentHandler):\n",
        "\tdef __init__(self):\n",
        "\t\txml.sax.ContentHandler.__init__(self)\n",
        "\t\tself.title_flag = False\n",
        "\t\tself.text_flag = False\n",
        "\t\tself.info_flag = False\n",
        "\t\tself.text_string = \"\"\n",
        "\n",
        "\tdef startElement(self, name, attrs):\n",
        "\t\tglobal doc_id\n",
        "\t\tif name == \"title\":\n",
        "\t\t\tself.title_flag = True\n",
        "\t\telif name == \"text\":\n",
        "\t\t\tself.text_flag = True\n",
        "\t\telif name == \"page\":\n",
        "\t\t\tdoc_id = doc_id + 1\n",
        "\n",
        "\tdef endElement(self, name):\n",
        "\t\tglobal stopword\n",
        "\t\tglobal doc_id\n",
        "\t\tglobal indexed_dict\n",
        "\t\tglobal output_path\n",
        "\t\tglobal index_number\n",
        "\n",
        "\t\tinfo_tokens = []\n",
        "\t\tbody_tokens = []\n",
        "\t\tlink_tokens = []\n",
        "\t\tmatch_curl = []\n",
        "\t\tcategory_str = \"\"\n",
        "\t\trefer_str = \"\"\n",
        "\t\tinfo_flag = False\n",
        "\t\tequal_flag = False\n",
        "\t\tn_flag = False\n",
        "\t\tbody_flag = True\n",
        "\t\tlink_flag = False\n",
        "\n",
        "\t\tif name == \"text\":\n",
        "\t\t\tself.text_flag = False\n",
        "\n",
        "\t\t\t# tokens = word_tokenize(self.text_string.replace(\"External links\", \"Externallinks\"))#.split(\" \")\n",
        "\t\t\ttokens = regtok(self.text_string.replace(\"External links\", \"Externallinks\"))\n",
        "\t\t\t\n",
        "\t\t\tcat_str = re.findall('(?<=\\[\\[category:)(.*?)(?=\\]\\])', self.text_string)\n",
        "\t\t\tref_str_1 = re.findall('(?<=\\* \\[\\[)(.*?)(?=\\])', self.text_string)\n",
        "\t\t\tref_str_2 = re.findall('(?<=\\* \\{\\{)(.*?)(?=\\}\\})', self.text_string)\n",
        "\n",
        "\t\t\tcategory_tokens = []\n",
        "\t\t\tfor stri in cat_str:\n",
        "\t\t\t\tif stri != '':\n",
        "\t\t\t\t\tcategory_tokens.append(stri.replace(\")\", \"\").replace(\"(\", \"\").strip())\n",
        "\n",
        "\t\t\trefer_tokens = []\n",
        "\t\t\tfor stri in ref_str_1:\n",
        "\t\t\t\ttemp = stri.replace(\")\", \"\").replace(\"(\", \"\").strip()\n",
        "\t\t\t\tif temp.isalpha():\t\n",
        "\t\t\t\t\trefer_tokens.append(temp)\n",
        "\n",
        "\t\t\tfor stri in ref_str_2:\n",
        "\t\t\t\ttemp = stri.replace(\")\", \"\").replace(\"(\", \"\").strip()\n",
        "\t\t\t\tif temp.isalpha():\t\n",
        "\t\t\t\t\trefer_tokens.append(temp)\n",
        "\n",
        "\t\t\tfor token in tokens:\n",
        "\t\t\t\tif token == '{{infobox':\n",
        "\t\t\t\t\tinfo_flag = True\n",
        "\t\t\t\t\tbody_flag = False\n",
        "\n",
        "\t\t\t\tif token == 'externallinks':\n",
        "\t\t\t\t\tlink_flag = True\n",
        "\t\t\t\t\tbody_flag = False\n",
        "\t\t\t\t\n",
        "\t\t\t\tif info_flag is False and link_flag is False:\n",
        "\t\t\t\t\tbody_flag = True\n",
        "\t\t\t\t\n",
        "\t\t\t\tif info_flag is True:\n",
        "\t\t\t\t\tif token == '}}' and n_flag is True:\n",
        "\t\t\t\t\t\tinfo_flag = False\n",
        "\t\t\t\t\tif n_flag is True:\n",
        "\t\t\t\t\t\tn_flag = False\n",
        "\t\t\t\t\tif token == '\\n':\n",
        "\t\t\t\t\t\tn_flag = True\n",
        "\t\t\t\t\tif token == '|':\n",
        "\t\t\t\t\t\tequal_flag = False\n",
        "\t\t\t\t\tif equal_flag is True:\n",
        "\t\t\t\t\t\tinfo_tokens.append(token)\n",
        "\t\t\t\t\tif token == '=':\n",
        "\t\t\t\t\t\tequal_flag = True\n",
        "\n",
        "\t\t\t\tif body_flag is True:\n",
        "\t\t\t\t\tbody_tokens.append(token.replace(\")\", \"\").replace(\"(\", \"\"))\n",
        "\t\t \n",
        "\t\t\t\tif link_flag is True and info_flag is False:\n",
        "\t\t\t\t\tlink_tokens.append(token.replace(\")\", \"\").replace(\"(\", \"\"))\n",
        "\n",
        "\t\t\t# Body\n",
        "\t\t\trefer_tokens = [stem(word) for word in refer_tokens if word.isalpha() and word not in stopword]\n",
        "\t\t\t\n",
        "\t\t\tfor key in refer_tokens:\n",
        "\t\t\t\tif key not in indexed_dict:\n",
        "\t\t\t\t\tindexed_dict[key] = [[], [], [], [], [], [], [0, 0, 0, 0, 0, 0]]\n",
        "\t\t\t\t\n",
        "\t\t\t\tindexed_dict[key][6][4] += 1\n",
        "\t\t\t\tif not indexed_dict[key][4] or indexed_dict[key][4][-1] != doc_id:\n",
        "\t\t\t\t\tindexed_dict[key][4].append(doc_id)\n",
        "\n",
        "\t\t\tcategory_tokens = [stem(word).strip() for word in category_tokens if word.isalpha() and word not in stopword]\n",
        "\n",
        "\t\t\tfor key in category_tokens:\n",
        "\t\t\t\tif key not in indexed_dict:\n",
        "\t\t\t\t\tindexed_dict[key] = [[], [], [], [], [], [], [0, 0, 0, 0, 0, 0]]\n",
        "\t\t\t\t\n",
        "\t\t\t\tindexed_dict[key][6][3] += 1\n",
        "\t\t\t\tif not indexed_dict[key][3] or indexed_dict[key][3][-1] != doc_id:\n",
        "\t\t\t\t\tindexed_dict[key][3].append(doc_id)\n",
        "\t\t\n",
        "\t\t\tbody_tokens = [stem(word).strip() for word in body_tokens if word.isalpha() and word not in stopword]\n",
        "\n",
        "\t\t\tfor key in body_tokens:\n",
        "\t\t\t\tif key not in indexed_dict:\n",
        "\t\t\t\t\tindexed_dict[key] = [[], [], [], [], [], [], [0, 0, 0, 0, 0, 0]]\n",
        "\t\t\t\t\n",
        "\t\t\t\tindexed_dict[key][6][2] += 1\n",
        "\t\t\t\tif not indexed_dict[key][2] or indexed_dict[key][2][-1] != doc_id:\n",
        "\t\t\t\t\tindexed_dict[key][2].append(doc_id)\n",
        "\t\t\n",
        "\t\t\tlink_tokens = [stem(word).strip() for word in link_tokens if word.isalpha() and word not in stopword]\n",
        "\n",
        "\t\t\tfor key in link_tokens:\n",
        "\t\t\t\tif key not in indexed_dict:\n",
        "\t\t\t\t\tindexed_dict[key] = [[], [], [], [], [], [], [0, 0, 0, 0, 0, 0]]\n",
        "\t\t\t\t\n",
        "\t\t\t\tindexed_dict[key][6][5] += 1\n",
        "\t\t\t\tif not indexed_dict[key][5] or indexed_dict[key][5][-1] != doc_id:\n",
        "\t\t\t\t\tindexed_dict[key][5].append(doc_id)\n",
        "\n",
        "\t\t\t# Infobox\n",
        "\t\t\tinfo_tokens = [stem(word).strip() for word in info_tokens if word.isalpha() and word not in stopword]\n",
        "\n",
        "\t\t\tfor key in info_tokens:\n",
        "\t\t\t\tif key not in indexed_dict:\n",
        "\t\t\t\t\tindexed_dict[key] = [[], [], [], [], [], [], [0, 0, 0, 0, 0, 0]]\n",
        "\t\t\t\t\n",
        "\t\t\t\tindexed_dict[key][6][1] += 1\n",
        "\t\t\t\tif not indexed_dict[key][1] or indexed_dict[key][1][-1] != doc_id:\n",
        "\t\t\t\t\tindexed_dict[key][1].append(doc_id)\n",
        "\n",
        "\t\t\tself.text_string = \"\"\n",
        "\n",
        "\tdef characters(self, data):\n",
        "\t\tglobal stopword\n",
        "\t\tglobal doc_id\n",
        "\t\tglobal indexed_dict\n",
        "\n",
        "\t\tif self.title_flag is True:\n",
        "\t\t\tdata = data.lower().replace(\")\", \"\").replace(\"(\", \"\").replace(\":\", \" \")\n",
        "\t\t\t# tokens = word_tokenize(data)#.split(\" \")\n",
        "\t\t\ttokens = regtok(data)\n",
        "\n",
        "\t\t\ttokens = [stem(word).strip() for word in tokens if word.isalpha() and word not in stopword]\n",
        "\t\t\t\n",
        "\t\t\tfor key in tokens:\n",
        "\t\t\t\tif key not in indexed_dict:\n",
        "\t\t\t\t\tindexed_dict[key] = [[], [], [], [], [], [], [0, 0, 0, 0, 0, 0]]\n",
        "\t\t\t\t\n",
        "\t\t\t\tindexed_dict[key][6][0] += 1\n",
        "\t\t\t\tif not indexed_dict[key][0] or indexed_dict[key][0][-1] != doc_id:\n",
        "\t\t\t\t\tindexed_dict[key][0].append(doc_id)\n",
        "\t\t\t\n",
        "\t\t\tself.title_flag = False\n",
        "\n",
        "\t\tif self.text_flag is True:\n",
        "\t\t\tdata = data.replace(\"External links\", \"Externallinks\").lower()\n",
        "\t\t\tself.text_string = self.text_string + \" \" + data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoqsHfcHOHI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path_zipped = 'gdrive/My Drive/ph1.xml-p1p30303.bz2'\n",
        "# !bzip2 -d 'gdrive/My Drive/ph1.xml-p1p30303.bz2'\n",
        "data_path_unzipped = 'gdrive/My Drive/ph1.xml-p1p30303'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTDPPXTCXQhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cutoff = int(1000)\n",
        "handler = NewHandler()\n",
        "parser = xml.sax.make_parser()\n",
        "parser.setContentHandler(handler)\n",
        "\n",
        "# For Running on the Zipped Version of the Dump:\n",
        "# for line in subprocess.Popen(['bzcat'], stdin = open(data_path), stdout = subprocess.PIPE).stdout:\n",
        "#     parser.feed(line)\n",
        "\n",
        "#     if doc_id > cutoff:\n",
        "#       break\n",
        "\n",
        "# For Running on the UnZipped Dump:\n",
        "# xml.sax.parse(data_path_unzipped, handler)\n",
        "\n",
        "file_handle = open('gdrive/My Drive/ph1.xml-p1p30303', 'r+')\n",
        "start_load = time.time()\n",
        "lines = file_handle.readlines()\n",
        "end_load = time.time()\n",
        "# size = len(lines)\n",
        "line = -1\n",
        "doc_id = 0\n",
        "count = 0\n",
        "\n",
        "# while count < size:\n",
        "while line != '':\n",
        "    # Get next line from file \n",
        "    line = lines[count]\n",
        "    # line = file_handle.readline()\n",
        "    parser.feed(line)\n",
        "    if doc_id > cutoff:\n",
        "      break\n",
        "    count += 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4FMwVUzcdDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_out = open(\"index.pkl\", \"wb\")\n",
        "pickle.dump(indexed_dict, pickle_out)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7lDWAu6hSBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e16c83f5-a1be-42ae-8a43-fbdaf531e032"
      },
      "source": [
        "end = time.time()\n",
        "print(\"Number of Docs Processed =\", doc_id)\n",
        "print(\"Time Taken = \" + str((end - start) / 60) + \" minutes\")\n",
        "# print(\"Load Time = \" + str((end_load - start_load) / 60) + \" minutes\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Docs Processed = 1001\n",
            "Time Taken = 0.5332383910814921 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92tkf6QyY0dQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "02128f82-6744-4e15-b5ab-2fb1e74ebe61"
      },
      "source": [
        "cnt = 0\n",
        "for key in indexed_dict.keys():\n",
        "  cnt += 1\n",
        "  if cnt < 10:\n",
        "    print(str(key) + \": \" + str(indexed_dict[key][6]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accessiblecomput: [1, 0, 0, 0, 0, 0]\n",
            "redirect: [0, 19, 274, 0, 0, 1]\n",
            "anarch: [1, 0, 170, 0, 0, 4]\n",
            "refend: [0, 0, 0, 0, 1, 0]\n",
            "libertarian: [0, 2, 95, 1, 0, 4]\n",
            "philosophi: [1, 271, 163, 0, 1, 0]\n",
            "move: [0, 372, 355, 0, 0, 2]\n",
            "dmi: [0, 41, 327, 0, 0, 1]\n",
            "british: [0, 484, 422, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ZBk0Uhcf4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a3fea29a-a53a-4b04-c040-4ae700957ed3"
      },
      "source": [
        "print(str((end - start) / 60) + \" minutes\")\n",
        "cnt = 0\n",
        "for key in indexed_dict.keys():\n",
        "  cnt += 1\n",
        "print(\"Number of Keys: \" + str(cnt))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5332383910814921 minutes\n",
            "Number of Keys: 74741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ0aB0k4neBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_in = open(\"index.pkl\", \"rb\")\n",
        "index = pickle.load(pickle_in)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov3yNEKOO4Lq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "c20dc89a-7daf-4005-d217-0555251588fd"
      },
      "source": [
        "# title: 0, infobox: 1, body: 2, categories: 3, references: 4, external_links: 5\n",
        "\n",
        "def field_query(query):\n",
        "  # t:World Cup i:2019 c:Cricket\n",
        "  tokens = regtok(query.replace(\"t:\", \"\").replace(\"i:\", \"\").replace(\"r:\", \"\").replace(\"l:\", \"\").replace(\"b:\", \"\").replace(\"c:\", \"\").lower())\n",
        "  tokens = [stem(word).strip() for word in tokens if word.isalpha() and word not in stopword]\n",
        "  for token in tokens:\n",
        "    if token not in index:\n",
        "      print(\"Token: \" + str(token) + \" NOT FOUND\")\n",
        "      continue\n",
        "    \n",
        "    print(\"Token: \" + str(token))\n",
        "    print(\"Counts:\")\n",
        "    print(\"Title: \" + str(index[token][6][0]) + \", Infobox: \" + str(index[token][6][1]) + \", Categories: \" + str(index[token][6][3]) + \", References: \" + str(index[token][6][4]) + \", Body: \" + str(index[token][6][2]) + \", Links: \" + str(index[token][6][5]))\n",
        "    print(\"Postings:\")\n",
        "    print(\"Title: \" + str(index[token][0]))\n",
        "    print(\"Infobox: \" + str(index[token][1]))\n",
        "    print(\"Categories: \" + str(index[token][3]))\n",
        "    print(\"References: \" + str(index[token][4]))\n",
        "    print(\"Body: \" + str(index[token][2]))\n",
        "    print(\"Links: \" + str(index[token][5]))\n",
        "    print()\n",
        "\n",
        "# field_query(\"t:World Cup i:2019 c:Cricket\")\n",
        "field_query(\"t: refend\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: refend\n",
            "Counts:\n",
            "Title: 0, Infobox: 0, Categories: 0, References: 1, Body: 0, Links: 0\n",
            "Postings:\n",
            "Title: []\n",
            "Infobox: []\n",
            "Categories: []\n",
            "References: [2]\n",
            "Body: []\n",
            "Links: []\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1d-DGreIjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}